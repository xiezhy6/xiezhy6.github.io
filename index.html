<!DOCTYPE html>
<head>
    <title>Zhenyu Xie</title>
    <meta name="author" content="Zhenyu Xie">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta property="og:title" content="Zhenyu Xie">
	<meta property="og:description" content="PhD student, Sun Yat-sen University">
    <meta property="og:image" content="https://xiezhy6.github.io/files/me.png">
	<meta property="og:url" content="https://xiezhy6.github.io/">
	<meta name="twitter:card" content="summary_large_image">
    <link rel="apple-touch-icon" href="files/12.jpg">
    <link rel="icon" type="image/jpg" href="files/12.jpg">
    <link rel="manifest" href="files/site.webmanifest">
    <link rel="stylesheet" href="style.css">
</head>

<div class="header noselect">
    <div class="content row">
        <div class="header-profile-picture"></div>
        <div class="header-text">
            <div class="header-name">
                <h1>Zhenyu Xie</h1>
            </div>
            <div class="header-subtitle">
                PhD student, Sun Yat-sen University
            </div>
            <div class="header-links">
                <a class="btn" href="#contact">Email</a> /
                <a class="btn" href="https://scholar.google.com/citations?user=a6D7UxwAAAAJ&hl=en">Google Scholar</a> /
                <a class="btn" href="https://github.com/xiezhy6">GitHub</a> /
                <!-- <a class="btn" href="https://twitter.com/ncklashansen">Twitter</a> / -->
                <!-- <a class="btn" href="https://www.linkedin.com/in/ncklas">LinkedIn</a> / -->
                <a class="btn" href="files/cv.pdf">CV</a>
            </div>
        </div>
    </div>
</div>
<div class="content" style="padding-bottom: 64px;">
    <div>
        <p>
            I am a PhD student at <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>, advised by Prof. <a href="https://lemondan.github.io/">Xiaodan Liang</a> 
            at <a href="https://www.sysu-hcp.net/">Human Cyber Physical Intelligence Integration Lab (HCP-I2 Lab)</a>. 
            I am currently a visting PhD student in the Robotics Institute at <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, advised by Prof. <a href="https://www.cs.cmu.edu/~ftorre/">Fernando de la Torre</a>.
            I was a research intern at <a href="https://www.bytedance.com/en/">ByteDance</a> and <a href="https://www.tencent.com/en-us/">Tencent</a> during my PhD study.
            Before starting my PhD, I received my BS and MS degrees from <a href="https://www.sysu.edu.cn/sysuen/">Sun Yat-sen University</a>, 
            advised by Prof. <a href="https://cse.sysu.edu.cn/content/2498">Jianhuang Lai</a> and Prof. <a href="https://cse.sysu.edu.cn/content/2478">Xiaohua Xie</a>.
            My research interests lie in human-centric 2D/3D modeling and 3D generative model.
            Currently, I am focusing on high-fidelty 3D human avatar modeling and efficient human-centric video generation.
        </p>
    </div>

    <div class="section-spacing">
        <div>
            <h2 class="noselect">Recent Activities</h2>
            <ul>
            <li class="list-item-spacing" ><span class="bold">[07/2024]</span> &nbsp; One paper (First author) about image-based 3D virtual try-on is accepted by <a href="">ACM MM 2024</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[01/2024]</span> &nbsp; One paper about text-to-motion synthesis is accepted by <a href="https://arxiv.org/pdf/2401.02142">TVCG 2024</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[12/2023]</span> &nbsp; One paper (First author) about text-to-motion synthesis is accepted by <a href="https://arxiv.org/pdf/2312.10960">AAAI 2024</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[11/2023]</span> &nbsp; Glad to serve as a visiting scholar in the Robotics Institute at CMU.</li>
            <li class="list-item-spacing" ><span class="bold">[03/2023]</span> &nbsp; One paper (First author) about general purpose garment-to-person virtual try-on is accepted by <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf">CVPR 2023</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[09/2022]</span> &nbsp; One paper about hard pose person-to-person virtual try-on is accepted by <a href="https://proceedings.neurips.cc/paper_files/paper/2022/file/d3221cdb27e49d9c1cd35ad254feccfe-Paper-Conference.pdf">NeurIPS 2022</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[06/2022]</span> &nbsp; One paper about cross-modal fashion image synthesis is accepted by <a href="https://arxiv.org/pdf/2208.05621">ACM MM 2022</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[03/2022]</span> &nbsp; One paper about in-the-wild person-to-person virtual try-on is accepted by <a href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Dressing_in_the_Wild_by_Watching_Dance_Videos_CVPR_2022_paper.pdf">CVPR 2022</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[10/2021]</span> &nbsp; One paper (First author) about unpaired person-to-person virtual try-on is accepted by <a href="https://proceedings.neurips.cc/paper_files/paper/2021/file/151de84cca69258b17375e2f44239191-Paper.pdf">NeurIPS 2021</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[10/2021]</span> &nbsp; One paper (Co-first author) about dance video synthesis is accepted by <a href="https://arxiv.org/pdf/2110.14147">TIP 2021</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[08/2021]</span> &nbsp; One paper about image-based 3D virtual try-on is accepted by <a href="https://arxiv.org/pdf/2108.00386">ICCV 2021</a>.</li>
            <li class="list-item-spacing" ><span class="bold">[07/2021]</span> &nbsp; One paper (First author) about Neural Architecture Search for garment-to-person virtual try-on is accepted by <a href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_M3D-VTON_A_Monocular-to-3D_Virtual_Try-On_Network_ICCV_2021_paper.pdf">ACM MM 2021</a>.</li>
            <!-- Add more list items here if needed -->
            </ul>
        </div>
        </div>

    <div>
        <h2 class="noselect">Publications and preprints</h2>
        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/dreamvton.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/pdf/2407.16511">DreamVTON: Customizing 3D Virtual Try-on with Personalized Diffusion Models</a><br/>
                <span class="bold">Zhenyu Xie</span>, Haoye Dong, Yufei Gao, Zehua Ma, Xiaodan Liang<br/>
                <span class="italic">ACM MM</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2407.16511">arXiv</a> / <a class="btn" href="">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/guess.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/pdf/2401.02142">GUESS:GradUally Enriching SyntheSis for Text-Driven Human Motion Generation</a><br/>
                Xuehao Gao, Yang Yang, <span class="bold">Zhenyu Xie</span>, Shaoyi Du, Zhongqian Sun, Yang Wu<br/>
                <span class="italic">TVCG</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2401.02142">arXiv</a> / <a class="btn" href="https://github.com/Xuehao-Gao/GUESS">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/b2a-hdm.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/pdf/2312.10960">Towards Detailed Text-to-Motion Synthesis via Basic-to-Advanced Hierarchical Diffusion Model</a><br/>
                <span class="bold">Zhenyu Xie</span>, Yang Wu, Xuehao Gao, Zhongqian Sun, Wei Yang, Xiaodan Liang<br/>
                <span class="italic">AAAI</span>, 2024<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2312.10960">arXiv</a> / <a class="btn" href="https://github.com/xiezhy6/B2A-HDM">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/gp-vton.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf">GP-VTON: Towards General Purpose Virtual Try-On via Collaborative Local-Flow Global-Parsing Learning</a><br/>
                <span class="bold">Zhenyu Xie</span>, Zaiyu Huang, Xin Dong, Fuwei Zhao, Haoye Dong, Xijin Zhang, Feida Zhu, Xiaodan<br/>
                <span class="italic">CVPR</span>, 2023<br/>
                <a class="btn btn-red" href="https://openaccess.thecvf.com/content/CVPR2023/papers/Xie_GP-VTON_Towards_General_Purpose_Virtual_Try-On_via_Collaborative_Local-Flow_Global-Parsing_CVPR_2023_paper.pdf">arXiv</a> / <a class="btn" href="https://github.com/xiezhy6/GP-VTON">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/3d-gcl.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/d3221cdb27e49d9c1cd35ad254feccfe-Paper-Conference.pdf">Towards Hard-pose Virtual Try-on via 3D-aware Global Correspondence Learning</a><br/>
                Zaiyu Huang, Hanhui Li, <span class="bold">Zhenyu Xie</span>, Michael Kampffmeyer, qingling Cai, Xiaodan Liang<br/>
                <span class="italic">NeurIPS</span>, 2022<br/>
                <a class="btn btn-red" href="https://proceedings.neurips.cc/paper_files/paper/2022/file/d3221cdb27e49d9c1cd35ad254feccfe-Paper-Conference.pdf">arXiv</a> / <a class="btn" href="https://github.com/huangzy225/3D-GCL">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/armani.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/pdf/2208.05621">Armani: Part-level garment-text alignment for unified cross-modal fashion design</a><br/>
                Xujie Zhang, Yu Sha, Michael C. Kampffmeyer, <span class="bold">Zhenyu Xie</span>, Zequn Jie, Chengwen Huang, Jianqing Peng, Xiaodan Liang<br/>
                <span class="italic">ACM MM</span>, 2022<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2208.05621">arXiv</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/wflow.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Dressing_in_the_Wild_by_Watching_Dance_Videos_CVPR_2022_paper.pdf">Dressing in the Wild by Watching Dance Videos</a><br/>
                Xin Dong, Fuwei Zhao, <span class="bold">Zhenyu Xie</span>, Xijin Zhang, Daniel K. Du, Min Zheng, Xiang Long, Xiaodan Liang, Jianchao Yang<br/>
                <span class="italic">CVPR</span>, 2022<br/>
                <a class="btn btn-red" href="https://openaccess.thecvf.com/content/CVPR2022/papers/Dong_Dressing_in_the_Wild_by_Watching_Dance_Videos_CVPR_2022_paper.pdf">arXiv</a> / <a class="btn" href="https://awesome-wflow.github.io/">project</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/pasta-gan.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/151de84cca69258b17375e2f44239191-Paper.pdf">Towards scalable unpaired virtual try-on via patch-routed spatially-adaptive gan</a><br/>
                <span class="bold">Zhenyu Xie</span>, Zaiyu Huang, Fuwei Zhao, Haoye Dong, Michael Kampffmeyer, Xiaodan Liang<br/>
                <span class="italic">NeurIPS</span>, 2021<br/>
                <a class="btn btn-red" href="https://proceedings.neurips.cc/paper_files/paper/2021/file/151de84cca69258b17375e2f44239191-Paper.pdf">arXiv</a> / <a class="btn" href="https://github.com/xiezhy6/PASTA-GAN">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/cpf-net.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/pdf/2110.14147">Image comes dancing with collaborative parsing-flow video synthesis</a><br/>
                Bowen Wu*, <span class="bold">Zhenyu Xie*</span>, Xiaodan Liang, Yubei Xiao, Haoye Dong, Liang Lin<br/>
                <span class="italic">TIP</span>, 2021<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2110.14147">arXiv</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/was-vton.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://arxiv.org/pdf/2108.00386">Was-vton: Warping architecture search for virtual try-on network</a><br/>
                <span class="bold">Zhenyu Xie</span>, Xujie Zhang, Fuwei Zhao, Haoye Dong, Michael C Kampffmeyer, Haonan Yan, Xiaodan Liang<br/>
                <span class="italic">ACM MM</span>, 2021<br/>
                <a class="btn btn-red" href="https://arxiv.org/pdf/2108.00386">arXiv</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/m3d-vton.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_M3D-VTON_A_Monocular-to-3D_Virtual_Try-On_Network_ICCV_2021_paper.pdf">M3d-vton: A monocular-to-3d virtual try-on network</a><br/>
                Fuwei Zhao, <span class="bold">Zhenyu Xie</span>, Michael Kampffmeyer, Haoye Dong, Songfang Han, Tianxiang Zheng, Tao Zhang, Xiaodan Liang<br/>
                <span class="italic">ICCV</span>, 2021<br/>
                <a class="btn btn-red" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Zhao_M3D-VTON_A_Monocular-to-3D_Virtual_Try-On_Network_ICCV_2021_paper.pdf">arXiv</a> / <a class="btn" href="https://github.com/fyviezhao/M3D-VTON">code</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

        <div class="publication row clearfix">
            <div class="row-media" style="background-image: url(files/fe-gan.png);"></div>
            <div class="row-text">
                <a class="publication-title bold" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Fashion_Editing_With_Adversarial_Parsing_Learning_CVPR_2020_paper.pdf">Fashion editing with adversarial parsing learning</a><br/>
                Haoye Dong, Xiaodan Liang, Yixuan Zhang, Xujie Zhang, Xiaohui Shen, <span class="bold">Zhenyu Xie</span>, Bowen Wu, Jian Yin<br/>
                <span class="italic">CVPR</span>, 2020<br/>
                <a class="btn btn-red" href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dong_Fashion_Editing_With_Adversarial_Parsing_Learning_CVPR_2020_paper.pdf">arXiv</a> / <a class="btn btn-dark" href="">bibtex</a>
            </div>
        </div>

    </div>

    <div class="section-spacing">
        <div>
            <h2 class="noselect">Academic Services</h2>
            <span class="bold">Organizer for Workshop</span><br/>
            <ul>
            <li class="list-item-spacing" >CVPR 2020 Workshop on Human-centric Image/Video Synthesis <a href="https://vuhcs.github.io/">L.I.P</a></li>
            <!-- Add more list items here if needed -->
            </ul>
            <span class="bold">Reviewer for Journal</span><br/>
            <ul>
            <li class="list-item-spacing" >International Journal of Computer Vision (IJCV)</li>
            <li class="list-item-spacing" >Neural Networks</li>
            <!-- Add more list items here if needed -->
            </ul>
            <span class="bold">Reviewer for Conference</span><br/>
            <ul>
            <li class="list-item-spacing" >IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</li>
            <li class="list-item-spacing" >International Conference on Computer Vision (ICCV)</li>
            <li class="list-item-spacing" >European Conference on Computer Vision ECCV (ECCV)</li>
            <li class="list-item-spacing" >Annual Conference on Neural Information Processing Systems (NeurIPS)</li>
            <!-- Add more list items here if needed -->
            </ul>
        </div>
        </div>

    <div class="noselect">
        <a id="contact"></a>
        <h2>Contact</h2>
        You are very welcome to contact me regarding my research. I typically respond within a few days.<br/>
        I can be contacted directly at <span class="bold">xiezhy6</span> [at] <span class="bold">mail2.sysu.edu</span>.cn
    </div>
</div>
<div class="footer noselect">
    <div class="footer-content">
        &copy; Thansk for website desidn from Nicklas Hansen <a style="color: white; text-decoration: underline;" href="https://github.com/nicklashansen/nicklashansen.github.io">here</a>.
    </div>
</div>
